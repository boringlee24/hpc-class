{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/24 22:01:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf().setMaster(\"local[*]\").setAppName(\"HIGGS\")\n",
    "conf.set(\"spark.executor.memory\", \"20G\")\n",
    "conf.set(\"spark.driver.memory\", \"2G\")\n",
    "conf.set(\"spark.executor.cores\", \"4\")\n",
    "conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "conf.set(\"spark.default.parallelism\", \"4\")\n",
    "# create a Spark Session instead of a Spark Context\n",
    "spark = SparkSession.builder \\\n",
    "    .config(conf = conf) \\\n",
    "  .appName(\"spark session example\") \\\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/24 22:02:39 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                 _c0|                 _c1|                 _c2|                 _c3|                 _c4|                 _c5|                 _c6|                 _c7|                 _c8|                 _c9|                _c10|                _c11|                _c12|                _c13|                _c14|                _c15|                _c16|                _c17|                _c18|                _c19|                _c20|                _c21|                _c22|                _c23|                _c24|                _c25|                _c26|                _c27|                _c28|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|1.000000000000000...|8.692932128906250...|-6.35081827640533...|2.256902605295181...|3.274700641632080...|-6.89993202686309...|7.542022466659545...|-2.48573139309883...|-1.09206390380859...|0.000000000000000...|1.374992132186889...|-6.53674185276031...|9.303491115570068...|1.107436060905456...|1.138904333114624...|-1.57819831371307...|-1.04698538780212...|0.000000000000000...|6.579295396804809...|-1.04545699432492...|-4.57671694457530...|3.101961374282836...|1.353760004043579...|9.795631170272827...|9.780761599540710...|9.200048446655273...|7.216574549674987...|9.887509346008300...|8.766783475875854...|\n",
      "|1.000000000000000...|9.075421094894409...|3.291472792625427...|3.594118654727935...|1.497969865798950...|-3.13009530305862...|1.095530629158020...|-5.57524919509887...|-1.58822977542877...|2.173076152801513...|8.125811815261840...|-2.13641926646232...|1.271014571189880...|2.214872121810913...|4.999939501285552...|-1.26143181324005...|7.321561574935913...|0.000000000000000...|3.987008929252624...|-1.13893008232116...|-8.19110195152461...|0.000000000000000...|3.022198975086212...|8.330481648445129...|9.856996536254882...|9.780983924865722...|7.797321677207946...|9.923557639122009...|7.983425855636596...|\n",
      "|1.000000000000000...|7.988347411155700...|1.470638751983642...|-1.63597476482391...|4.537731707096099...|4.256291687488555...|1.104874610900878...|1.282322287559509...|1.381664276123046...|0.000000000000000...|8.517372012138366...|1.540658950805664...|-8.19689512252807...|2.214872121810913...|9.934899210929870...|3.560801148414611...|-2.08777546882629...|2.548224449157714...|1.256954550743103...|1.128847599029541...|9.004608392715454...|0.000000000000000...|9.097532629966735...|1.108330488204956...|9.856922030448913...|9.513312578201293...|8.032515048980712...|8.659244179725646...|7.801175713539123...|\n",
      "|0.000000000000000...|1.344384789466857...|-8.76626014709472...|9.359127283096313...|1.992050051689147...|8.824543952941894...|1.786065936088562...|-1.64677774906158...|-9.42382514476776...|0.000000000000000...|2.423264741897583...|-6.76015794277191...|7.361586689949035...|2.214872121810913...|1.298719763755798...|-1.43073809146881...|-3.64658176898956...|0.000000000000000...|7.453126907348632...|-6.78378820419311...|-1.36035633087158...|0.000000000000000...|9.466524720191955...|1.028703689575195...|9.986560940742492...|7.282806038856506...|8.692002296447753...|1.026736497879028...|9.579039812088012...|\n",
      "|1.000000000000000...|1.105008959770202...|3.213555514812469...|1.522401213645935...|8.828076124191284...|-1.20534932613372...|6.814661026000976...|-1.07046389579772...|-9.21870648860931...|0.000000000000000...|8.008721470832824...|1.020974040031433...|9.714065194129943...|2.214872121810913...|5.967612862586975...|-3.50272864103317...|6.311942934989929...|0.000000000000000...|4.799988865852355...|-3.73565524816513...|1.130406111478805...|0.000000000000000...|7.558564543724060...|1.361057043075561...|9.866096973419189...|8.380846381187438...|1.133295178413391...|8.722448945045471...|8.084865212440490...|\n",
      "|0.000000000000000...|1.595839262008666...|-6.07810676097869...|7.074915803968906...|1.818449616432189...|-1.11905992031097...|8.475499153137207...|-5.66437005996704...|1.581239342689514...|2.173076152801513...|7.554209828376770...|6.431096196174621...|1.426366806030273...|0.000000000000000...|9.216607809066772...|-1.19043242931365...|-1.61558902263641...|0.000000000000000...|6.511141061782836...|-6.54226958751678...|-1.27434492111206...|3.101961374282836...|8.237605690956115...|9.381914138793945...|9.717581868171691...|7.891763448715209...|4.305532872676849...|9.613569378852844...|9.578179121017456...|\n",
      "|1.000000000000000...|4.093913435935974...|-1.88468360900878...|-1.02729201316833...|1.672451734542846...|-1.60459828376770...|1.338014960289001...|5.542744323611259...|1.346588134765625...|2.173076152801513...|5.097832679748535...|-1.03833806514739...|7.078623175621032...|0.000000000000000...|7.469175457954406...|-3.58465105295181...|-1.64665424823760...|0.000000000000000...|3.670579791069030...|6.949646025896072...|1.377130270004272...|3.101961374282836...|8.694183826446533...|1.222082972526550...|1.000627398490905...|5.450449585914611...|6.986525058746337...|9.773144721984863...|8.287860751152038...|\n",
      "|1.000000000000000...|9.338953495025634...|6.291297078132629...|5.275348424911499...|2.380327433347702...|-9.66569125652313...|5.478111505508422...|-5.94392269849777...|-1.70686614513397...|2.173076152801513...|9.410027265548706...|-2.65373277664184...|-1.57219991087913...|0.000000000000000...|1.030370354652404...|-1.75505101680755...|5.230209231376647...|2.548224449157714...|1.373546600341796...|1.291248083114624...|-1.46745443344116...|0.000000000000000...|9.018372893333435...|1.083671212196350...|9.796960949897766...|7.833003997802734...|8.491951823234558...|8.943563103675842...|7.748793959617614...|\n",
      "|1.000000000000000...|1.405143737792968...|5.366026163101196...|6.895543336868286...|1.179567337036132...|-1.10061153769493...|3.202404975891113...|-1.52696001529693...|-1.57603347301483...|0.000000000000000...|2.931536912918090...|5.673424601554870...|-1.30033344030380...|2.214872121810913...|1.787122726440429...|8.994985818862915...|5.851513147354125...|2.548224449157714...|4.018652141094207...|-1.51201695203781...|1.163489103317260...|0.000000000000000...|1.667070508003234...|4.039272785186767...|1.175828456878662...|1.045351743698120...|1.542971968650817...|3.534826755523681...|2.740753889083862...|\n",
      "|1.000000000000000...|1.176565527915954...|1.041605025529861...|1.397002458572387...|4.797213077545166...|2.655133903026580...|1.135563015937805...|1.534830927848815...|-2.53291219472885...|0.000000000000000...|1.027246594429016...|5.343157649040222...|1.180022358894348...|0.000000000000000...|2.405661106109619...|8.755676448345184...|-9.76534068584442...|2.548224449157714...|1.250382542610168...|2.685412168502807...|5.303344726562500...|0.000000000000000...|8.331748843193054...|7.739681005477905...|9.857499599456787...|1.103696346282958...|8.491398692131042...|9.371039867401123...|8.123638033866882...|\n",
      "|1.000000000000000...|9.459739923477172...|1.111244320869445...|1.218337059020996...|9.076390862464904...|8.215369582176208...|1.153243303298950...|-3.65420281887054...|-1.56605482101440...|0.000000000000000...|7.447192072868347...|7.208195328712463...|-3.75822931528091...|2.214872121810913...|6.088791489601135...|3.078369498252868...|-1.28163838386535...|0.000000000000000...|1.597967982292175...|-4.51018035411834...|6.365344673395156...|3.101961374282836...|8.290241360664367...|9.806482791900634...|9.943597912788391...|9.082478284835815...|7.758789062500000...|7.833113670349121...|7.251217961311340...|\n",
      "|0.000000000000000...|7.393567562103271...|-1.78290426731109...|8.299342393875122...|5.045390725135803...|-1.30216747522354...|9.610513448715209...|-3.55517983436584...|-1.71739935874938...|2.173076152801513...|6.209560632705688...|-4.81741040945053...|-1.19919323921203...|0.000000000000000...|9.826014041900634...|8.118502795696258...|-2.90323644876480...|0.000000000000000...|1.064662933349609...|7.740649580955505...|3.988203406333923...|3.101961374282836...|9.445360302925109...|1.026260614395141...|9.821967482566833...|5.421146750450134...|1.250978946685791...|8.300446271896362...|7.613079547882080...|\n",
      "|1.000000000000000...|1.384097695350646...|1.168220937252044...|-1.17987895011901...|7.629125714302062...|-7.97822698950767...|1.019863128662109...|8.773182630538940...|1.276887178421020...|2.173076152801513...|3.312520980834960...|1.409523487091064...|-1.47438883781433...|0.000000000000000...|1.282738208770751...|7.374743819236755...|-2.25419610738754...|0.000000000000000...|1.559753060340881...|8.465205430984497...|5.048085451126098...|3.101961374282836...|9.593246579170227...|8.073760271072387...|1.191813588142395...|1.221210360527038...|8.611412644386291...|9.293408989906311...|8.383023738861083...|\n",
      "|1.000000000000000...|1.383548736572265...|8.891792893409729...|6.185320615768432...|1.081547021865844...|3.446055650711059...|9.563793540000915...|8.545429706573486...|-1.12920701503753...|2.173076152801513...|5.456657409667968...|-3.07865172624588...|-6.23279809951782...|2.214872121810913...|3.482571244239807...|1.024202585220336...|1.840776652097702...|0.000000000000000...|7.813369035720825...|-1.63612556457519...|1.144067287445068...|0.000000000000000...|5.222384929656982...|7.376385331153869...|9.861995577812194...|1.349615693092346...|8.127878904342651...|9.534064531326293...|7.797226309776306...|\n",
      "|1.000000000000000...|1.343652725219726...|8.385329246520996...|-1.06113851070404...|2.472015142440795...|-5.72631716728210...|1.512709975242614...|1.143690109252929...|8.555619716644287...|0.000000000000000...|8.842203021049499...|1.474605560302734...|-1.36064875125885...|1.107436060905456...|1.587265610694885...|2.234833478927612...|7.756848633289337...|0.000000000000000...|1.609408140182495...|2.396404743194580...|7.572935223579406...|0.000000000000000...|9.340201020240783...|8.447072505950927...|1.077844023704528...|1.400183677673339...|9.477745294570922...|1.007614254951477...|9.010174870491027...|\n",
      "|0.000000000000000...|5.470141768455505...|-3.49708944559097...|-6.46657168865203...|2.040462255477905...|2.764569818973541...|5.446965098381042...|8.386992812156677...|1.728703141212463...|0.000000000000000...|6.528096199035644...|1.471691370010375...|1.243273019790649...|0.000000000000000...|7.857298851013183...|-4.44292910397052...|-1.01980340480804...|2.548224449157714...|4.191471040248870...|-6.29242181777954...|1.570794582366943...|3.101961374282836...|6.894335746765136...|8.672295808792114...|1.082487821578979...|6.641419529914855...|3.541145622730255...|5.799450278282165...|8.172734379768371...|\n",
      "|1.000000000000000...|1.484203696250915...|1.699521422386169...|-1.05947399139404...|2.700195550918579...|-1.05596387386322...|2.409452915191650...|4.574607908725738...|3.449823260307312...|0.000000000000000...|1.414903521537780...|1.114225864410400...|-1.44886660575866...|0.000000000000000...|1.012983918190002...|-2.05698895454406...|1.131010890007019...|0.000000000000000...|9.054746031761169...|2.182368993759155...|1.043073177337646...|0.000000000000000...|1.653626322746276...|9.935762286186218...|9.833217859268188...|7.413797974586486...|1.633816361427307...|5.923243165016174...|7.451378703117370...|\n",
      "|0.000000000000000...|1.057975649833679...|-1.60759001970291...|-1.94997251033782...|2.705023050308227...|-7.51476705074310...|1.909918904304504...|-1.03184497356414...|8.649863600730895...|0.000000000000000...|1.300834894180297...|1.467376798391342...|-1.11874294281005...|1.107436060905456...|9.669710993766784...|-3.66657346487045...|1.108266711235046...|0.000000000000000...|5.547249317169189...|-7.14190185070037...|1.505314946174621...|3.101961374282836...|9.544943571090698...|6.510385870933532...|1.124949693679809...|8.940010070800781...|6.721734404563903...|1.182358264923095...|1.316304087638854...|\n",
      "|0.000000000000000...|6.753035783767700...|1.120983958244323...|-2.80445903539657...|1.539554953575134...|7.345175743103027...|6.146844029426574...|-5.07023155689239...|7.945806980133056...|2.173076152801513...|2.188202738761901...|-1.89411830902099...|-5.80557882785797...|0.000000000000000...|1.245682120323181...|-3.47542107105255...|-8.56156468391418...|2.548224449157714...|7.531017661094665...|-1.14559268951416...|-1.37478399276733...|0.000000000000000...|9.069401025772094...|8.983390927314758...|1.119651079177856...|1.269073486328125...|1.088765859603881...|1.015413045883178...|9.146358966827392...|\n",
      "|1.000000000000000...|6.427279114723205...|-1.42984032630920...|1.519071936607360...|9.409985542297363...|8.872274160385131...|1.615126848220825...|-1.33683574199676...|-2.66596227884292...|1.086538076400756...|1.667088270187377...|6.557375192642211...|-1.58812892436981...|0.000000000000000...|8.282302021980285...|1.836144566535949...|4.081907570362091...|0.000000000000000...|1.708718180656433...|-3.46915155649185...|-1.18278455734252...|3.101961374282836...|9.210902452468872...|1.373361706733703...|9.849172830581665...|1.422878146171569...|1.546551108360290...|1.782585501670837...|1.438173770904541...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"delimiter\", \",\").option(\"header\", \"false\").csv('/work/li.baol/data/HIGGS.csv')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_c0',\n",
       " '_c1',\n",
       " '_c2',\n",
       " '_c3',\n",
       " '_c4',\n",
       " '_c5',\n",
       " '_c6',\n",
       " '_c7',\n",
       " '_c8',\n",
       " '_c9',\n",
       " '_c10',\n",
       " '_c11',\n",
       " '_c12',\n",
       " '_c13',\n",
       " '_c14',\n",
       " '_c15',\n",
       " '_c16',\n",
       " '_c17',\n",
       " '_c18',\n",
       " '_c19',\n",
       " '_c20',\n",
       " '_c21',\n",
       " '_c22',\n",
       " '_c23',\n",
       " '_c24',\n",
       " '_c25',\n",
       " '_c26',\n",
       " '_c27',\n",
       " '_c28']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:===>                                                    (16 + 8) / 237]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/24 21:52:45 WARN BlockManager: Putting block rdd_10_22 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/work/li.baol/spark-3.3.1-bin-hadoop2/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/work/li.baol/spark-3.3.1-bin-hadoop2/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/work/li.baol/spark-3.3.1-bin-hadoop2/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/work/li.baol/spark-3.3.1-bin-hadoop2/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/work/li.baol/spark-3.3.1-bin-hadoop2/python/pyspark/mllib/util.py\", line 144, in <lambda>\n",
      "    parsed = lines.map(lambda l: MLUtils._parse_libsvm_line(l))\n",
      "  File \"/work/li.baol/spark-3.3.1-bin-hadoop2/python/lib/pyspark.zip/pyspark/mllib/util.py\", line 61, in _parse_libsvm_line\n",
      "    index, value = items[1 + i].split(\":\")\n",
      "ValueError: not enough values to unpack (expected 2, got 1)\n",
      ".\n",
      "22/11/24 21:52:45 WARN BlockManager: Block rdd_10_22 could not be removed as it was not found on disk or in memory\n",
      "22/11/24 21:52:45 ERROR Executor: Exception in task 22.0 in stage 2.0 (TID 72)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/work/li.baol/spark-3.3.1-bin-hadoop2/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/work/li.baol/spark-3.3.1-bin-hadoop2/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/work/li.baol/spark-3.3.1-bin-hadoop2/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/work/li.baol/spark-3.3.1-bin-hadoop2/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/work/li.baol/spark-3.3.1-bin-hadoop2/python/pyspark/mllib/util.py\", line 144, in <lambda>\n",
      "    parsed = lines.map(lambda l: MLUtils._parse_libsvm_line(l))\n",
      "  File \"/work/li.baol/spark-3.3.1-bin-hadoop2/python/lib/pyspark.zip/pyspark/mllib/util.py\", line 61, in _parse_libsvm_line\n",
      "    index, value = items[1 + i].split(\":\")\n",
      "ValueError: not enough values to unpack (expected 2, got 1)\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsBytes(MemoryStore.scala:352)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1535)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1445)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1509)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1332)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "22/11/24 21:52:45 WARN TaskSetManager: Lost task 22.0 in stage 2.0 (TID 72) (c0172 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/work/li.baol/spark-3.3.1-bin-hadoop2/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/work/li.baol/spark-3.3.1-bin-hadoop2/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/work/li.baol/spark-3.3.1-bin-hadoop2/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/work/li.baol/spark-3.3.1-bin-hadoop2/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/work/li.baol/spark-3.3.1-bin-hadoop2/python/pyspark/mllib/util.py\", line 144, in <lambda>\n",
      "    parsed = lines.map(lambda l: MLUtils._parse_libsvm_line(l))\n",
      "  File \"/work/li.baol/spark-3.3.1-bin-hadoop2/python/lib/pyspark.zip/pyspark/mllib/util.py\", line 61, in _parse_libsvm_line\n",
      "    index, value = items[1 + i].split(\":\")\n",
      "ValueError: not enough values to unpack (expected 2, got 1)\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsBytes(MemoryStore.scala:352)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1535)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1445)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1509)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1332)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "22/11/24 21:52:45 ERROR TaskSetManager: Task 22 in stage 2.0 failed 1 times; aborting job\n",
      "22/11/24 21:52:45 WARN BlockManager: Putting block rdd_10_21 failed due to exception org.apache.spark.TaskKilledException.\n",
      "22/11/24 21:52:45 WARN BlockManager: Block rdd_10_21 could not be removed as it was not found on disk or in memory\n",
      "22/11/24 21:52:45 WARN BlockManager: Putting block rdd_10_16 failed due to exception org.apache.spark.TaskKilledException.\n",
      "22/11/24 21:52:45 WARN BlockManager: Block rdd_10_16 could not be removed as it was not found on disk or in memory\n",
      "22/11/24 21:52:45 WARN TaskSetManager: Lost task 16.0 in stage 2.0 (TID 66) (c0172 executor driver): TaskKilled (Stage cancelled)\n",
      "22/11/24 21:52:45 WARN BlockManager: Putting block rdd_10_23 failed due to exception org.apache.spark.TaskKilledException.\n",
      "22/11/24 21:52:45 WARN BlockManager: Block rdd_10_23 could not be removed as it was not found on disk or in memory\n",
      "22/11/24 21:52:45 WARN BlockManager: Putting block rdd_10_17 failed due to exception org.apache.spark.TaskKilledException.\n",
      "22/11/24 21:52:45 WARN BlockManager: Block rdd_10_17 could not be removed as it was not found on disk or in memory\n",
      "22/11/24 21:52:45 WARN BlockManager: Putting block rdd_10_18 failed due to exception org.apache.spark.TaskKilledException.\n",
      "22/11/24 21:52:45 WARN TaskSetManager: Lost task 21.0 in stage 2.0 (TID 71) (c0172 executor driver): TaskKilled (Stage cancelled)\n",
      "22/11/24 21:52:45 WARN BlockManager: Block rdd_10_18 could not be removed as it was not found on disk or in memory\n",
      "22/11/24 21:52:45 WARN TaskSetManager: Lost task 23.0 in stage 2.0 (TID 73) (c0172 executor driver): TaskKilled (Stage cancelled)\n",
      "22/11/24 21:52:45 WARN TaskSetManager: Lost task 17.0 in stage 2.0 (TID 67) (c0172 executor driver): TaskKilled (Stage cancelled)\n",
      "22/11/24 21:52:45 WARN BlockManager: Putting block rdd_10_20 failed due to exception org.apache.spark.TaskKilledException.\n",
      "22/11/24 21:52:45 WARN BlockManager: Block rdd_10_20 could not be removed as it was not found on disk or in memory\n",
      "22/11/24 21:52:45 WARN BlockManager: Putting block rdd_10_19 failed due to exception org.apache.spark.TaskKilledException.\n",
      "22/11/24 21:52:45 WARN BlockManager: Block rdd_10_19 could not be removed as it was not found on disk or in memory\n",
      "22/11/24 21:52:45 WARN TaskSetManager: Lost task 18.0 in stage 2.0 (TID 68) (c0172 executor driver): TaskKilled (Stage cancelled)\n",
      "22/11/24 21:52:45 WARN BlockManager: Putting block rdd_10_24 failed due to exception org.apache.spark.TaskKilledException.\n",
      "22/11/24 21:52:45 WARN BlockManager: Block rdd_10_24 could not be removed as it was not found on disk or in memory\n",
      "22/11/24 21:52:45 WARN TaskSetManager: Lost task 19.0 in stage 2.0 (TID 69) (c0172 executor driver): TaskKilled (Stage cancelled)\n",
      "22/11/24 21:52:45 WARN TaskSetManager: Lost task 20.0 in stage 2.0 (TID 70) (c0172 executor driver): TaskKilled (Stage cancelled)\n",
      "22/11/24 21:52:45 WARN TaskSetManager: Lost task 24.0 in stage 2.0 (TID 74) (c0172 executor driver): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 22 in stage 2.0 failed 1 times, most recent failure: Lost task 22.0 in stage 2.0 (TID 72) (c0172 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/work/li.baol/spark-3.3.1-bin-hadoop2/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/work/li.baol/spark-3.3.1-bin-hadoop2/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/work/li.baol/spark-3.3.1-bin-hadoop2/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/work/li.baol/spark-3.3.1-bin-hadoop2/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/work/li.baol/spark-3.3.1-bin-hadoop2/python/pyspark/mllib/util.py\", line 144, in <lambda>\n    parsed = lines.map(lambda l: MLUtils._parse_libsvm_line(l))\n  File \"/work/li.baol/spark-3.3.1-bin-hadoop2/python/lib/pyspark.zip/pyspark/mllib/util.py\", line 61, in _parse_libsvm_line\n    index, value = items[1 + i].split(\":\")\nValueError: not enough values to unpack (expected 2, got 1)\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsBytes(MemoryStore.scala:352)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1535)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1445)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1509)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1332)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/work/li.baol/spark-3.3.1-bin-hadoop2/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/work/li.baol/spark-3.3.1-bin-hadoop2/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/work/li.baol/spark-3.3.1-bin-hadoop2/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/work/li.baol/spark-3.3.1-bin-hadoop2/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/work/li.baol/spark-3.3.1-bin-hadoop2/python/pyspark/mllib/util.py\", line 144, in <lambda>\n    parsed = lines.map(lambda l: MLUtils._parse_libsvm_line(l))\n  File \"/work/li.baol/spark-3.3.1-bin-hadoop2/python/lib/pyspark.zip/pyspark/mllib/util.py\", line 61, in _parse_libsvm_line\n    index, value = items[1 + i].split(\":\")\nValueError: not enough values to unpack (expected 2, got 1)\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsBytes(MemoryStore.scala:352)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1535)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1445)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1509)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1332)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/work/li.baol/GIT/hpc-class/spark_project/higgs.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22446973636f766572794e6f646531227d/work/li.baol/GIT/hpc-class/spark_project/higgs.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Load and parse the data file into an RDD of LabeledPoint.\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22446973636f766572794e6f646531227d/work/li.baol/GIT/hpc-class/spark_project/higgs.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m data \u001b[39m=\u001b[39m MLUtils\u001b[39m.\u001b[39;49mloadLibSVMFile(sc, \u001b[39m'\u001b[39;49m\u001b[39m/work/li.baol/data/HIGGS.txt\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m/work/li.baol/spark-3.3.1-bin-hadoop2/python/pyspark/mllib/util.py:147\u001b[0m, in \u001b[0;36mMLUtils.loadLibSVMFile\u001b[0;34m(sc, path, numFeatures, minPartitions)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[39mif\u001b[39;00m numFeatures \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    146\u001b[0m     parsed\u001b[39m.\u001b[39mcache()\n\u001b[0;32m--> 147\u001b[0m     numFeatures \u001b[39m=\u001b[39m parsed\u001b[39m.\u001b[39;49mmap(\u001b[39mlambda\u001b[39;49;00m x: \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m \u001b[39mif\u001b[39;49;00m x[\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39;49msize \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m x[\u001b[39m1\u001b[39;49m][\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m])\u001b[39m.\u001b[39;49mreduce(\u001b[39mmax\u001b[39;49m) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    148\u001b[0m \u001b[39mreturn\u001b[39;00m parsed\u001b[39m.\u001b[39mmap(\n\u001b[1;32m    149\u001b[0m     \u001b[39mlambda\u001b[39;00m x: LabeledPoint(\n\u001b[1;32m    150\u001b[0m         x[\u001b[39m0\u001b[39m], Vectors\u001b[39m.\u001b[39msparse(numFeatures, x[\u001b[39m1\u001b[39m], x[\u001b[39m2\u001b[39m])  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    151\u001b[0m     )\n\u001b[1;32m    152\u001b[0m )\n",
      "File \u001b[0;32m/work/li.baol/spark-3.3.1-bin-hadoop2/python/pyspark/rdd.py:1250\u001b[0m, in \u001b[0;36mRDD.reduce\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m   1247\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   1248\u001b[0m     \u001b[39myield\u001b[39;00m reduce(f, iterator, initial)\n\u001b[0;32m-> 1250\u001b[0m vals \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmapPartitions(func)\u001b[39m.\u001b[39;49mcollect()\n\u001b[1;32m   1251\u001b[0m \u001b[39mif\u001b[39;00m vals:\n\u001b[1;32m   1252\u001b[0m     \u001b[39mreturn\u001b[39;00m reduce(f, vals)\n",
      "File \u001b[0;32m/work/li.baol/spark-3.3.1-bin-hadoop2/python/pyspark/rdd.py:1197\u001b[0m, in \u001b[0;36mRDD.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1195\u001b[0m \u001b[39mwith\u001b[39;00m SCCallSiteSync(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontext):\n\u001b[1;32m   1196\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1197\u001b[0m     sock_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mctx\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mPythonRDD\u001b[39m.\u001b[39;49mcollectAndServe(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jrdd\u001b[39m.\u001b[39;49mrdd())\n\u001b[1;32m   1198\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(_load_from_socket(sock_info, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch1.10/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch1.10/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 22 in stage 2.0 failed 1 times, most recent failure: Lost task 22.0 in stage 2.0 (TID 72) (c0172 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/work/li.baol/spark-3.3.1-bin-hadoop2/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/work/li.baol/spark-3.3.1-bin-hadoop2/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/work/li.baol/spark-3.3.1-bin-hadoop2/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/work/li.baol/spark-3.3.1-bin-hadoop2/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/work/li.baol/spark-3.3.1-bin-hadoop2/python/pyspark/mllib/util.py\", line 144, in <lambda>\n    parsed = lines.map(lambda l: MLUtils._parse_libsvm_line(l))\n  File \"/work/li.baol/spark-3.3.1-bin-hadoop2/python/lib/pyspark.zip/pyspark/mllib/util.py\", line 61, in _parse_libsvm_line\n    index, value = items[1 + i].split(\":\")\nValueError: not enough values to unpack (expected 2, got 1)\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsBytes(MemoryStore.scala:352)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1535)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1445)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1509)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1332)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/work/li.baol/spark-3.3.1-bin-hadoop2/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/work/li.baol/spark-3.3.1-bin-hadoop2/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/work/li.baol/spark-3.3.1-bin-hadoop2/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/work/li.baol/spark-3.3.1-bin-hadoop2/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/work/li.baol/spark-3.3.1-bin-hadoop2/python/pyspark/mllib/util.py\", line 144, in <lambda>\n    parsed = lines.map(lambda l: MLUtils._parse_libsvm_line(l))\n  File \"/work/li.baol/spark-3.3.1-bin-hadoop2/python/lib/pyspark.zip/pyspark/mllib/util.py\", line 61, in _parse_libsvm_line\n    index, value = items[1 + i].split(\":\")\nValueError: not enough values to unpack (expected 2, got 1)\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsBytes(MemoryStore.scala:352)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1535)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1445)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1509)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1332)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "# Load and parse the data file into an RDD of LabeledPoint.\n",
    "data = MLUtils.loadLibSVMFile(sc, '/work/li.baol/data/HIGGS.txt')\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "# (trainingData, testData) = data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Mean Squared Error = 0.1\n",
      "Learned regression tree model:\n",
      "DecisionTreeModel regressor of depth 2 with 5 nodes\n",
      "  If (feature 405 <= 21.0)\n",
      "   If (feature 100 <= 193.5)\n",
      "    Predict: 0.0\n",
      "   Else (feature 100 > 193.5)\n",
      "    Predict: 1.0\n",
      "  Else (feature 405 > 21.0)\n",
      "   Predict: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train a DecisionTree model.\n",
    "#  Empty categoricalFeaturesInfo indicates all features are continuous.\n",
    "model = DecisionTree.trainRegressor(trainingData, categoricalFeaturesInfo={},\n",
    "                                    impurity='variance', maxDepth=5, maxBins=32)\n",
    "\n",
    "# Evaluate model on test instances and compute test error\n",
    "predictions = model.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "testMSE = labelsAndPredictions.map(lambda lp: (lp[0] - lp[1]) * (lp[0] - lp[1])).sum() /\\\n",
    "    float(testData.count())\n",
    "print('Test Mean Squared Error = ' + str(testMSE))\n",
    "print('Learned regression tree model:')\n",
    "print(model.toDebugString())\n",
    "\n",
    "# Save and load model\n",
    "# model.save(sc, \"target/tmp/myDecisionTreeRegressionModel\")\n",
    "# sameModel = DecisionTreeModel.load(sc, \"target/tmp/myDecisionTreeRegressionModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[31] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('pytorch1.10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4fc5fa8ed76756c9f1c1fe21d69b4a13ec4b696a25ed91a91b6a167c0ca20b72"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
