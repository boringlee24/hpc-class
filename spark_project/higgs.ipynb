{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.classification import LabeledPoint\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/03 23:55:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# when set local[*], all available cores will be used, the only executor is the driver\n",
    "conf = SparkConf().setMaster(\"local[*]\").setAppName(\"HIGGS\") \n",
    "conf.set(\"spark.executor.memory\", \"4G\")\n",
    "conf.set(\"spark.driver.memory\", \"20G\")\n",
    "# conf.set(\"spark.executor.cores\", \"4\")\n",
    "conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "# conf.set(\"spark.default.parallelism\", \"4\")\n",
    "# create a Spark Session instead of a Spark Context\n",
    "spark = SparkSession.builder \\\n",
    "    .config(conf = conf) \\\n",
    "  .appName(\"spark session example\") \\\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/03 23:56:07 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                 _c0|                 _c1|                 _c2|                 _c3|                 _c4|                 _c5|                 _c6|                 _c7|                 _c8|                 _c9|                _c10|                _c11|                _c12|                _c13|                _c14|                _c15|                _c16|                _c17|                _c18|                _c19|                _c20|                _c21|                _c22|                _c23|                _c24|                _c25|                _c26|                _c27|                _c28|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|1.000000000000000...|8.692932128906250...|-6.35081827640533...|2.256902605295181...|3.274700641632080...|-6.89993202686309...|7.542022466659545...|-2.48573139309883...|-1.09206390380859...|0.000000000000000...|1.374992132186889...|-6.53674185276031...|9.303491115570068...|1.107436060905456...|1.138904333114624...|-1.57819831371307...|-1.04698538780212...|0.000000000000000...|6.579295396804809...|-1.04545699432492...|-4.57671694457530...|3.101961374282836...|1.353760004043579...|9.795631170272827...|9.780761599540710...|9.200048446655273...|7.216574549674987...|9.887509346008300...|8.766783475875854...|\n",
      "|1.000000000000000...|9.075421094894409...|3.291472792625427...|3.594118654727935...|1.497969865798950...|-3.13009530305862...|1.095530629158020...|-5.57524919509887...|-1.58822977542877...|2.173076152801513...|8.125811815261840...|-2.13641926646232...|1.271014571189880...|2.214872121810913...|4.999939501285552...|-1.26143181324005...|7.321561574935913...|0.000000000000000...|3.987008929252624...|-1.13893008232116...|-8.19110195152461...|0.000000000000000...|3.022198975086212...|8.330481648445129...|9.856996536254882...|9.780983924865722...|7.797321677207946...|9.923557639122009...|7.983425855636596...|\n",
      "|1.000000000000000...|7.988347411155700...|1.470638751983642...|-1.63597476482391...|4.537731707096099...|4.256291687488555...|1.104874610900878...|1.282322287559509...|1.381664276123046...|0.000000000000000...|8.517372012138366...|1.540658950805664...|-8.19689512252807...|2.214872121810913...|9.934899210929870...|3.560801148414611...|-2.08777546882629...|2.548224449157714...|1.256954550743103...|1.128847599029541...|9.004608392715454...|0.000000000000000...|9.097532629966735...|1.108330488204956...|9.856922030448913...|9.513312578201293...|8.032515048980712...|8.659244179725646...|7.801175713539123...|\n",
      "|0.000000000000000...|1.344384789466857...|-8.76626014709472...|9.359127283096313...|1.992050051689147...|8.824543952941894...|1.786065936088562...|-1.64677774906158...|-9.42382514476776...|0.000000000000000...|2.423264741897583...|-6.76015794277191...|7.361586689949035...|2.214872121810913...|1.298719763755798...|-1.43073809146881...|-3.64658176898956...|0.000000000000000...|7.453126907348632...|-6.78378820419311...|-1.36035633087158...|0.000000000000000...|9.466524720191955...|1.028703689575195...|9.986560940742492...|7.282806038856506...|8.692002296447753...|1.026736497879028...|9.579039812088012...|\n",
      "|1.000000000000000...|1.105008959770202...|3.213555514812469...|1.522401213645935...|8.828076124191284...|-1.20534932613372...|6.814661026000976...|-1.07046389579772...|-9.21870648860931...|0.000000000000000...|8.008721470832824...|1.020974040031433...|9.714065194129943...|2.214872121810913...|5.967612862586975...|-3.50272864103317...|6.311942934989929...|0.000000000000000...|4.799988865852355...|-3.73565524816513...|1.130406111478805...|0.000000000000000...|7.558564543724060...|1.361057043075561...|9.866096973419189...|8.380846381187438...|1.133295178413391...|8.722448945045471...|8.084865212440490...|\n",
      "|0.000000000000000...|1.595839262008666...|-6.07810676097869...|7.074915803968906...|1.818449616432189...|-1.11905992031097...|8.475499153137207...|-5.66437005996704...|1.581239342689514...|2.173076152801513...|7.554209828376770...|6.431096196174621...|1.426366806030273...|0.000000000000000...|9.216607809066772...|-1.19043242931365...|-1.61558902263641...|0.000000000000000...|6.511141061782836...|-6.54226958751678...|-1.27434492111206...|3.101961374282836...|8.237605690956115...|9.381914138793945...|9.717581868171691...|7.891763448715209...|4.305532872676849...|9.613569378852844...|9.578179121017456...|\n",
      "|1.000000000000000...|4.093913435935974...|-1.88468360900878...|-1.02729201316833...|1.672451734542846...|-1.60459828376770...|1.338014960289001...|5.542744323611259...|1.346588134765625...|2.173076152801513...|5.097832679748535...|-1.03833806514739...|7.078623175621032...|0.000000000000000...|7.469175457954406...|-3.58465105295181...|-1.64665424823760...|0.000000000000000...|3.670579791069030...|6.949646025896072...|1.377130270004272...|3.101961374282836...|8.694183826446533...|1.222082972526550...|1.000627398490905...|5.450449585914611...|6.986525058746337...|9.773144721984863...|8.287860751152038...|\n",
      "|1.000000000000000...|9.338953495025634...|6.291297078132629...|5.275348424911499...|2.380327433347702...|-9.66569125652313...|5.478111505508422...|-5.94392269849777...|-1.70686614513397...|2.173076152801513...|9.410027265548706...|-2.65373277664184...|-1.57219991087913...|0.000000000000000...|1.030370354652404...|-1.75505101680755...|5.230209231376647...|2.548224449157714...|1.373546600341796...|1.291248083114624...|-1.46745443344116...|0.000000000000000...|9.018372893333435...|1.083671212196350...|9.796960949897766...|7.833003997802734...|8.491951823234558...|8.943563103675842...|7.748793959617614...|\n",
      "|1.000000000000000...|1.405143737792968...|5.366026163101196...|6.895543336868286...|1.179567337036132...|-1.10061153769493...|3.202404975891113...|-1.52696001529693...|-1.57603347301483...|0.000000000000000...|2.931536912918090...|5.673424601554870...|-1.30033344030380...|2.214872121810913...|1.787122726440429...|8.994985818862915...|5.851513147354125...|2.548224449157714...|4.018652141094207...|-1.51201695203781...|1.163489103317260...|0.000000000000000...|1.667070508003234...|4.039272785186767...|1.175828456878662...|1.045351743698120...|1.542971968650817...|3.534826755523681...|2.740753889083862...|\n",
      "|1.000000000000000...|1.176565527915954...|1.041605025529861...|1.397002458572387...|4.797213077545166...|2.655133903026580...|1.135563015937805...|1.534830927848815...|-2.53291219472885...|0.000000000000000...|1.027246594429016...|5.343157649040222...|1.180022358894348...|0.000000000000000...|2.405661106109619...|8.755676448345184...|-9.76534068584442...|2.548224449157714...|1.250382542610168...|2.685412168502807...|5.303344726562500...|0.000000000000000...|8.331748843193054...|7.739681005477905...|9.857499599456787...|1.103696346282958...|8.491398692131042...|9.371039867401123...|8.123638033866882...|\n",
      "|1.000000000000000...|9.459739923477172...|1.111244320869445...|1.218337059020996...|9.076390862464904...|8.215369582176208...|1.153243303298950...|-3.65420281887054...|-1.56605482101440...|0.000000000000000...|7.447192072868347...|7.208195328712463...|-3.75822931528091...|2.214872121810913...|6.088791489601135...|3.078369498252868...|-1.28163838386535...|0.000000000000000...|1.597967982292175...|-4.51018035411834...|6.365344673395156...|3.101961374282836...|8.290241360664367...|9.806482791900634...|9.943597912788391...|9.082478284835815...|7.758789062500000...|7.833113670349121...|7.251217961311340...|\n",
      "|0.000000000000000...|7.393567562103271...|-1.78290426731109...|8.299342393875122...|5.045390725135803...|-1.30216747522354...|9.610513448715209...|-3.55517983436584...|-1.71739935874938...|2.173076152801513...|6.209560632705688...|-4.81741040945053...|-1.19919323921203...|0.000000000000000...|9.826014041900634...|8.118502795696258...|-2.90323644876480...|0.000000000000000...|1.064662933349609...|7.740649580955505...|3.988203406333923...|3.101961374282836...|9.445360302925109...|1.026260614395141...|9.821967482566833...|5.421146750450134...|1.250978946685791...|8.300446271896362...|7.613079547882080...|\n",
      "|1.000000000000000...|1.384097695350646...|1.168220937252044...|-1.17987895011901...|7.629125714302062...|-7.97822698950767...|1.019863128662109...|8.773182630538940...|1.276887178421020...|2.173076152801513...|3.312520980834960...|1.409523487091064...|-1.47438883781433...|0.000000000000000...|1.282738208770751...|7.374743819236755...|-2.25419610738754...|0.000000000000000...|1.559753060340881...|8.465205430984497...|5.048085451126098...|3.101961374282836...|9.593246579170227...|8.073760271072387...|1.191813588142395...|1.221210360527038...|8.611412644386291...|9.293408989906311...|8.383023738861083...|\n",
      "|1.000000000000000...|1.383548736572265...|8.891792893409729...|6.185320615768432...|1.081547021865844...|3.446055650711059...|9.563793540000915...|8.545429706573486...|-1.12920701503753...|2.173076152801513...|5.456657409667968...|-3.07865172624588...|-6.23279809951782...|2.214872121810913...|3.482571244239807...|1.024202585220336...|1.840776652097702...|0.000000000000000...|7.813369035720825...|-1.63612556457519...|1.144067287445068...|0.000000000000000...|5.222384929656982...|7.376385331153869...|9.861995577812194...|1.349615693092346...|8.127878904342651...|9.534064531326293...|7.797226309776306...|\n",
      "|1.000000000000000...|1.343652725219726...|8.385329246520996...|-1.06113851070404...|2.472015142440795...|-5.72631716728210...|1.512709975242614...|1.143690109252929...|8.555619716644287...|0.000000000000000...|8.842203021049499...|1.474605560302734...|-1.36064875125885...|1.107436060905456...|1.587265610694885...|2.234833478927612...|7.756848633289337...|0.000000000000000...|1.609408140182495...|2.396404743194580...|7.572935223579406...|0.000000000000000...|9.340201020240783...|8.447072505950927...|1.077844023704528...|1.400183677673339...|9.477745294570922...|1.007614254951477...|9.010174870491027...|\n",
      "|0.000000000000000...|5.470141768455505...|-3.49708944559097...|-6.46657168865203...|2.040462255477905...|2.764569818973541...|5.446965098381042...|8.386992812156677...|1.728703141212463...|0.000000000000000...|6.528096199035644...|1.471691370010375...|1.243273019790649...|0.000000000000000...|7.857298851013183...|-4.44292910397052...|-1.01980340480804...|2.548224449157714...|4.191471040248870...|-6.29242181777954...|1.570794582366943...|3.101961374282836...|6.894335746765136...|8.672295808792114...|1.082487821578979...|6.641419529914855...|3.541145622730255...|5.799450278282165...|8.172734379768371...|\n",
      "|1.000000000000000...|1.484203696250915...|1.699521422386169...|-1.05947399139404...|2.700195550918579...|-1.05596387386322...|2.409452915191650...|4.574607908725738...|3.449823260307312...|0.000000000000000...|1.414903521537780...|1.114225864410400...|-1.44886660575866...|0.000000000000000...|1.012983918190002...|-2.05698895454406...|1.131010890007019...|0.000000000000000...|9.054746031761169...|2.182368993759155...|1.043073177337646...|0.000000000000000...|1.653626322746276...|9.935762286186218...|9.833217859268188...|7.413797974586486...|1.633816361427307...|5.923243165016174...|7.451378703117370...|\n",
      "|0.000000000000000...|1.057975649833679...|-1.60759001970291...|-1.94997251033782...|2.705023050308227...|-7.51476705074310...|1.909918904304504...|-1.03184497356414...|8.649863600730895...|0.000000000000000...|1.300834894180297...|1.467376798391342...|-1.11874294281005...|1.107436060905456...|9.669710993766784...|-3.66657346487045...|1.108266711235046...|0.000000000000000...|5.547249317169189...|-7.14190185070037...|1.505314946174621...|3.101961374282836...|9.544943571090698...|6.510385870933532...|1.124949693679809...|8.940010070800781...|6.721734404563903...|1.182358264923095...|1.316304087638854...|\n",
      "|0.000000000000000...|6.753035783767700...|1.120983958244323...|-2.80445903539657...|1.539554953575134...|7.345175743103027...|6.146844029426574...|-5.07023155689239...|7.945806980133056...|2.173076152801513...|2.188202738761901...|-1.89411830902099...|-5.80557882785797...|0.000000000000000...|1.245682120323181...|-3.47542107105255...|-8.56156468391418...|2.548224449157714...|7.531017661094665...|-1.14559268951416...|-1.37478399276733...|0.000000000000000...|9.069401025772094...|8.983390927314758...|1.119651079177856...|1.269073486328125...|1.088765859603881...|1.015413045883178...|9.146358966827392...|\n",
      "|1.000000000000000...|6.427279114723205...|-1.42984032630920...|1.519071936607360...|9.409985542297363...|8.872274160385131...|1.615126848220825...|-1.33683574199676...|-2.66596227884292...|1.086538076400756...|1.667088270187377...|6.557375192642211...|-1.58812892436981...|0.000000000000000...|8.282302021980285...|1.836144566535949...|4.081907570362091...|0.000000000000000...|1.708718180656433...|-3.46915155649185...|-1.18278455734252...|3.101961374282836...|9.210902452468872...|1.373361706733703...|9.849172830581665...|1.422878146171569...|1.546551108360290...|1.782585501670837...|1.438173770904541...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"delimiter\", \",\").option(\"header\", \"false\").csv('/work/li.baol/data/HIGGS.csv')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.rdd.map(lambda row: LabeledPoint(float(row['_c0']), list(map(lambda x: float(x), row.asDict().values()))[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(1.0, [0.869293212890625,-0.6350818276405334,0.22569026052951813,0.327470064163208,-0.6899932026863098,0.7542022466659546,-0.24857313930988312,-1.0920639038085938,0.0,1.3749921321868896,-0.6536741852760315,0.9303491115570068,1.1074360609054565,1.138904333114624,-1.5781983137130737,-1.046985387802124,0.0,0.657929539680481,-0.010454569943249226,-0.0457671694457531,3.101961374282837,1.353760004043579,0.9795631170272827,0.978076159954071,0.9200048446655273,0.7216574549674988,0.9887509346008301,0.8766783475875854]),\n",
       " LabeledPoint(1.0, [0.9075421094894409,0.3291472792625427,0.3594118654727936,1.4979698657989502,-0.3130095303058624,1.09553062915802,-0.5575249195098877,-1.588229775428772,2.1730761528015137,0.8125811815261841,-0.2136419266462326,1.2710145711898804,2.214872121810913,0.4999939501285553,-1.2614318132400513,0.7321561574935913,0.0,0.39870089292526245,-1.138930082321167,-0.0008191101951524615,0.0,0.3022198975086212,0.8330481648445129,0.9856996536254883,0.9780983924865723,0.7797321677207947,0.9923557639122009,0.7983425855636597])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainingData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model = DecisionTree.trainClassifier(trainingData, numClasses=2, categoricalFeaturesInfo={},\n",
    "                                    impurity='gini', maxDepth=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[59] at mapPartitions at PythonMLLibAPI.scala:1344"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate model on test instances and compute test error\n",
    "predictions = model.predict(testData.map(lambda x: x.features))\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0],\n",
       " [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.take(15), testData.map(lambda x: x.label).take(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[62] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = testData.map(lambda lp: lp.label)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "predsAndLabels = testData.map(lambda lp: (model.predict(lp.features), lp.label))\n",
    "# predsAndLabels.collect()\n",
    "# labels = labels.distinct()\n",
    "# labels.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/work/li.baol/spark-3.3.1-bin-hadoop2/python/pyspark/serializers.py\", line 458, in dumps\n",
      "    return cloudpickle.dumps(obj, pickle_protocol)\n",
      "  File \"/work/li.baol/spark-3.3.1-bin-hadoop2/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\n",
      "    cp.dump(obj)\n",
      "  File \"/work/li.baol/spark-3.3.1-bin-hadoop2/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 602, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "  File \"/work/li.baol/spark-3.3.1-bin-hadoop2/python/pyspark/context.py\", line 447, in __getnewargs__\n",
      "    raise RuntimeError(\n",
      "RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Could not serialize object: RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/work/li.baol/spark-3.3.1-bin-hadoop2/python/pyspark/serializers.py:458\u001b[0m, in \u001b[0;36mCloudPickleSerializer.dumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 458\u001b[0m     \u001b[39mreturn\u001b[39;00m cloudpickle\u001b[39m.\u001b[39;49mdumps(obj, pickle_protocol)\n\u001b[1;32m    459\u001b[0m \u001b[39mexcept\u001b[39;00m pickle\u001b[39m.\u001b[39mPickleError:\n",
      "File \u001b[0;32m/work/li.baol/spark-3.3.1-bin-hadoop2/python/pyspark/cloudpickle/cloudpickle_fast.py:73\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, protocol, buffer_callback)\u001b[0m\n\u001b[1;32m     70\u001b[0m cp \u001b[39m=\u001b[39m CloudPickler(\n\u001b[1;32m     71\u001b[0m     file, protocol\u001b[39m=\u001b[39mprotocol, buffer_callback\u001b[39m=\u001b[39mbuffer_callback\n\u001b[1;32m     72\u001b[0m )\n\u001b[0;32m---> 73\u001b[0m cp\u001b[39m.\u001b[39;49mdump(obj)\n\u001b[1;32m     74\u001b[0m \u001b[39mreturn\u001b[39;00m file\u001b[39m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m/work/li.baol/spark-3.3.1-bin-hadoop2/python/pyspark/cloudpickle/cloudpickle_fast.py:602\u001b[0m, in \u001b[0;36mCloudPickler.dump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 602\u001b[0m     \u001b[39mreturn\u001b[39;00m Pickler\u001b[39m.\u001b[39;49mdump(\u001b[39mself\u001b[39;49m, obj)\n\u001b[1;32m    603\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/work/li.baol/spark-3.3.1-bin-hadoop2/python/pyspark/context.py:447\u001b[0m, in \u001b[0;36mSparkContext.__getnewargs__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getnewargs__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m NoReturn:\n\u001b[1;32m    446\u001b[0m     \u001b[39m# This method is called when attempting to pickle SparkContext, which is always an error:\u001b[39;00m\n\u001b[0;32m--> 447\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    448\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIt appears that you are attempting to reference SparkContext from a broadcast \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    449\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mvariable, action, or transformation. SparkContext can only be used on the driver, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    450\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mnot in code that it run on workers. For more information, see SPARK-5063.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    451\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/work/li.baol/GIT/hpc-class/spark_project/higgs.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22446973636f766572794e6f646531227d/work/li.baol/GIT/hpc-class/spark_project/higgs.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m metrics \u001b[39m=\u001b[39m BinaryClassificationMetrics(predsAndLabels)\n",
      "File \u001b[0;32m/work/li.baol/spark-3.3.1-bin-hadoop2/python/pyspark/mllib/evaluation.py:73\u001b[0m, in \u001b[0;36mBinaryClassificationMetrics.__init__\u001b[0;34m(self, scoreAndLabels)\u001b[0m\n\u001b[1;32m     71\u001b[0m sc \u001b[39m=\u001b[39m scoreAndLabels\u001b[39m.\u001b[39mctx\n\u001b[1;32m     72\u001b[0m sql_ctx \u001b[39m=\u001b[39m SQLContext\u001b[39m.\u001b[39mgetOrCreate(sc)\n\u001b[0;32m---> 73\u001b[0m numCol \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(scoreAndLabels\u001b[39m.\u001b[39;49mfirst())\n\u001b[1;32m     74\u001b[0m schema \u001b[39m=\u001b[39m StructType(\n\u001b[1;32m     75\u001b[0m     [\n\u001b[1;32m     76\u001b[0m         StructField(\u001b[39m\"\u001b[39m\u001b[39mscore\u001b[39m\u001b[39m\"\u001b[39m, DoubleType(), nullable\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m),\n\u001b[1;32m     77\u001b[0m         StructField(\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m, DoubleType(), nullable\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m),\n\u001b[1;32m     78\u001b[0m     ]\n\u001b[1;32m     79\u001b[0m )\n\u001b[1;32m     80\u001b[0m \u001b[39mif\u001b[39;00m numCol \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m:\n",
      "File \u001b[0;32m/work/li.baol/spark-3.3.1-bin-hadoop2/python/pyspark/rdd.py:1903\u001b[0m, in \u001b[0;36mRDD.first\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfirst\u001b[39m(\u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mRDD[T]\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[1;32m   1891\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1892\u001b[0m \u001b[39m    Return the first element in this RDD.\u001b[39;00m\n\u001b[1;32m   1893\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1901\u001b[0m \u001b[39m    ValueError: RDD is empty\u001b[39;00m\n\u001b[1;32m   1902\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1903\u001b[0m     rs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtake(\u001b[39m1\u001b[39;49m)\n\u001b[1;32m   1904\u001b[0m     \u001b[39mif\u001b[39;00m rs:\n\u001b[1;32m   1905\u001b[0m         \u001b[39mreturn\u001b[39;00m rs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/work/li.baol/spark-3.3.1-bin-hadoop2/python/pyspark/rdd.py:1883\u001b[0m, in \u001b[0;36mRDD.take\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1880\u001b[0m         taken \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1882\u001b[0m p \u001b[39m=\u001b[39m \u001b[39mrange\u001b[39m(partsScanned, \u001b[39mmin\u001b[39m(partsScanned \u001b[39m+\u001b[39m numPartsToTry, totalParts))\n\u001b[0;32m-> 1883\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcontext\u001b[39m.\u001b[39;49mrunJob(\u001b[39mself\u001b[39;49m, takeUpToNumLeft, p)\n\u001b[1;32m   1885\u001b[0m items \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m res\n\u001b[1;32m   1886\u001b[0m partsScanned \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m numPartsToTry\n",
      "File \u001b[0;32m/work/li.baol/spark-3.3.1-bin-hadoop2/python/pyspark/context.py:1486\u001b[0m, in \u001b[0;36mSparkContext.runJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   1484\u001b[0m mappedRDD \u001b[39m=\u001b[39m rdd\u001b[39m.\u001b[39mmapPartitions(partitionFunc)\n\u001b[1;32m   1485\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1486\u001b[0m sock_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mPythonRDD\u001b[39m.\u001b[39mrunJob(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jsc\u001b[39m.\u001b[39msc(), mappedRDD\u001b[39m.\u001b[39;49m_jrdd, partitions)\n\u001b[1;32m   1487\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(_load_from_socket(sock_info, mappedRDD\u001b[39m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m/work/li.baol/spark-3.3.1-bin-hadoop2/python/pyspark/rdd.py:3505\u001b[0m, in \u001b[0;36mPipelinedRDD._jrdd\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3502\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3503\u001b[0m     profiler \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 3505\u001b[0m wrapped_func \u001b[39m=\u001b[39m _wrap_function(\n\u001b[1;32m   3506\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mctx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prev_jrdd_deserializer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jrdd_deserializer, profiler\n\u001b[1;32m   3507\u001b[0m )\n\u001b[1;32m   3509\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   3510\u001b[0m python_rdd \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mPythonRDD(\n\u001b[1;32m   3511\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prev_jrdd\u001b[39m.\u001b[39mrdd(), wrapped_func, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreservesPartitioning, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_barrier\n\u001b[1;32m   3512\u001b[0m )\n",
      "File \u001b[0;32m/work/li.baol/spark-3.3.1-bin-hadoop2/python/pyspark/rdd.py:3362\u001b[0m, in \u001b[0;36m_wrap_function\u001b[0;34m(sc, func, deserializer, serializer, profiler)\u001b[0m\n\u001b[1;32m   3360\u001b[0m \u001b[39massert\u001b[39;00m serializer, \u001b[39m\"\u001b[39m\u001b[39mserializer should not be empty\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3361\u001b[0m command \u001b[39m=\u001b[39m (func, profiler, deserializer, serializer)\n\u001b[0;32m-> 3362\u001b[0m pickled_command, broadcast_vars, env, includes \u001b[39m=\u001b[39m _prepare_for_python_RDD(sc, command)\n\u001b[1;32m   3363\u001b[0m \u001b[39massert\u001b[39;00m sc\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   3364\u001b[0m \u001b[39mreturn\u001b[39;00m sc\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mPythonFunction(\n\u001b[1;32m   3365\u001b[0m     \u001b[39mbytearray\u001b[39m(pickled_command),\n\u001b[1;32m   3366\u001b[0m     env,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3371\u001b[0m     sc\u001b[39m.\u001b[39m_javaAccumulator,\n\u001b[1;32m   3372\u001b[0m )\n",
      "File \u001b[0;32m/work/li.baol/spark-3.3.1-bin-hadoop2/python/pyspark/rdd.py:3345\u001b[0m, in \u001b[0;36m_prepare_for_python_RDD\u001b[0;34m(sc, command)\u001b[0m\n\u001b[1;32m   3342\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_prepare_for_python_RDD\u001b[39m(sc: \u001b[39m\"\u001b[39m\u001b[39mSparkContext\u001b[39m\u001b[39m\"\u001b[39m, command: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[\u001b[39mbytes\u001b[39m, Any, Any, Any]:\n\u001b[1;32m   3343\u001b[0m     \u001b[39m# the serialized command will be compressed by broadcast\u001b[39;00m\n\u001b[1;32m   3344\u001b[0m     ser \u001b[39m=\u001b[39m CloudPickleSerializer()\n\u001b[0;32m-> 3345\u001b[0m     pickled_command \u001b[39m=\u001b[39m ser\u001b[39m.\u001b[39;49mdumps(command)\n\u001b[1;32m   3346\u001b[0m     \u001b[39massert\u001b[39;00m sc\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   3347\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(pickled_command) \u001b[39m>\u001b[39m sc\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mPythonUtils\u001b[39m.\u001b[39mgetBroadcastThreshold(sc\u001b[39m.\u001b[39m_jsc):  \u001b[39m# Default 1M\u001b[39;00m\n\u001b[1;32m   3348\u001b[0m         \u001b[39m# The broadcast will have same life cycle as created PythonRDD\u001b[39;00m\n",
      "File \u001b[0;32m/work/li.baol/spark-3.3.1-bin-hadoop2/python/pyspark/serializers.py:468\u001b[0m, in \u001b[0;36mCloudPickleSerializer.dumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    466\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mCould not serialize object: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (e\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, emsg)\n\u001b[1;32m    467\u001b[0m print_exec(sys\u001b[39m.\u001b[39mstderr)\n\u001b[0;32m--> 468\u001b[0m \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mPicklingError(msg)\n",
      "\u001b[0;31mPicklingError\u001b[0m: Could not serialize object: RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063."
     ]
    }
   ],
   "source": [
    "metrics = BinaryClassificationMetrics(predsAndLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_label = labels.zipWithIndex().filter(lambda x: x[1] < 100).keys()\n",
    "filter_pred = predictions.zipWithIndex().filter(lambda x: x[1] < 100).keys()\n",
    "predsAndLabels = filter_label.zip(filter_pred) #predictions.zip(labels)\n",
    "predsAndLabels.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_label.glom().map(len).collect(), filter_pred.glom().map(len).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Allocate more cores, and see if it's possible to directly construct an RDD. Or try a different serializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_label.zip(filter_label).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_pred.collect(), filter_label.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predsAndLabels.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testMSE = predsAndLabels.map(lambda lp: (lp[1] - lp[0]) * (lp[1] - lp[0])).sum() /\\\n",
    "#     float(test_data.count())\n",
    "metrics = BinaryClassificationMetrics(predsAndLabels)    \n",
    "print(f'AUC = {metrics.areaUnderROC}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('pytorch1.10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4fc5fa8ed76756c9f1c1fe21d69b4a13ec4b696a25ed91a91b6a167c0ca20b72"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
